22/03/17 16:35:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 1:>                                                          (0 + 0) / 8][Stage 1:>                                                          (0 + 8) / 8][Stage 0:>    (0 + 0) / 1][Stage 1:>    (0 + 8) / 8][Stage 5:>    (0 + 0) / 1][Stage 0:>    (0 + 1) / 1][Stage 1:>    (1 + 7) / 8][Stage 5:>    (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1][Stage 0:>                  (0 + 1) / 1][Stage 5:>                  (0 + 1) / 1]22/03/17 18:15:25 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 211, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 132, in dump_stream
    for obj in iterator:
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 200, in _batched
    for item in iterator:
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 450, in mapper
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 85, in <lambda>
    return lambda *a: f(*a)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py", line 73, in wrapper
    return f(*args, **kwargs)
  File "extract_and_deliver_telco_sms_indicators.py", line 21, in format_number
    is_civ = str_nbr.find("225")
NameError: name 'str_nbr' is not defined

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)
22/03/17 18:15:25 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 8) (beta-test-ai executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 211, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 132, in dump_stream
    for obj in iterator:
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 200, in _batched
    for item in iterator:
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 450, in mapper
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 85, in <lambda>
    return lambda *a: f(*a)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py", line 73, in wrapper
    return f(*args, **kwargs)
  File "extract_and_deliver_telco_sms_indicators.py", line 21, in format_number
    is_civ = str_nbr.find("225")
NameError: name 'str_nbr' is not defined

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)

22/03/17 18:15:25 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[Stage 0:>                  (0 + 0) / 1][Stage 5:>                  (0 + 1) / 1]generate_dates: right_date: iteration: 0 20220213
generate_dates: left_date: iteration: 0 20220207
generate_dates: right_date: iteration: 1 20220206
generate_dates: left_date: iteration: 1 20220131
generate_dates: right_date: iteration: 2 20220130
generate_dates: left_date: iteration: 2 20220124
generate_dates: right_date: iteration: 3 20220123
generate_dates: left_date: iteration: 3 20220117
generate_dates: right_date: iteration: 4 20220116
generate_dates: left_date: iteration: 4 20220110
generate_dates: right_date: iteration: 5 20220109
generate_dates: left_date: iteration: 5 20220103
generate_dates: right_date: iteration: 6 20220102
generate_dates: left_date: iteration: 6 20211227
generate_dates: right_date: iteration: 7 20211226
generate_dates: left_date: iteration: 7 20211220
generate_dates: right_date: iteration: 8 20211219
generate_dates: left_date: iteration: 8 20211213
generate_dates: right_date: iteration: 9 20211212
generate_dates: left_date: iteration: 9 20211206
generate_dates: right_date: iteration: 10 20211205
generate_dates: left_date: iteration: 10 20211129
generate_dates: right_date: iteration: 11 20211128
generate_dates: left_date: iteration: 11 20211122
generate_dates: right_date: iteration: 12 20211121
generate_dates: left_date: iteration: 12 20211115
generate_dates: right_date: iteration: 13 20211114
generate_dates: left_date: iteration: 13 20211108
generate_dates: right_date: iteration: 14 20211107
generate_dates: left_date: iteration: 14 20211101
generate_dates: right_date: iteration: 15 20211031
generate_dates: left_date: iteration: 15 20211025
generate_dates: right_date: iteration: 16 20211024
generate_dates: left_date: iteration: 16 20211018
generate_dates: right_date: iteration: 17 20211017
generate_dates: left_date: iteration: 17 20211011
generate_dates: right_date: iteration: 18 20211010
generate_dates: left_date: iteration: 18 20211004
generate_dates: right_date: iteration: 19 20211003
generate_dates: left_date: iteration: 19 20210927
generate_dates: right_date: iteration: 20 20210926
generate_dates: left_date: iteration: 20 20210920
generate_dates: right_date: iteration: 21 20210919
generate_dates: left_date: iteration: 21 20210913
generate_dates: right_date: iteration: 22 20210912
generate_dates: left_date: iteration: 22 20210906
generate_dates: right_date: iteration: 23 20210905
generate_dates: left_date: iteration: 23 20210830
generate_dates: right_date: iteration: 24 20210829
generate_dates: left_date: iteration: 24 20210823
generate_dates: right_date: iteration: 25 20210822
generate_dates: left_date: iteration: 25 20210816
generate_dates: right_date: iteration: 26 20210815
generate_dates: left_date: iteration: 26 20210809
generate_dates: right_date: iteration: 27 20210808
generate_dates: left_date: iteration: 27 20210802
generate_dates: right_date: iteration: 28 20210801
generate_dates: left_date: iteration: 28 20210726
generate_dates: right_date: iteration: 29 20210725
generate_dates: left_date: iteration: 29 20210719
generate_dates: right_date: iteration: 30 20210718
generate_dates: left_date: iteration: 30 20210712
generate_dates: right_date: iteration: 31 20210711
generate_dates: left_date: iteration: 31 20210705
generate_dates: right_date: iteration: 32 20210704
generate_dates: left_date: iteration: 32 20210628
generate_dates: right_date: iteration: 33 20210627
generate_dates: left_date: iteration: 33 20210621
generate_dates: right_date: iteration: 34 20210620
generate_dates: left_date: iteration: 34 20210614
generate_dates: right_date: iteration: 35 20210613
generate_dates: left_date: iteration: 35 20210607
generate_dates: right_date: iteration: 36 20210606
generate_dates: left_date: iteration: 36 20210531
generate_dates: right_date: iteration: 37 20210530
generate_dates: left_date: iteration: 37 20210524
generate_dates: right_date: iteration: 38 20210523
generate_dates: left_date: iteration: 38 20210517
generate_dates: right_date: iteration: 39 20210516
generate_dates: left_date: iteration: 39 20210510
generate_dates: right_date: iteration: 40 20210509
generate_dates: left_date: iteration: 40 20210503
generate_dates: right_date: iteration: 41 20210502
generate_dates: left_date: iteration: 41 20210426
generate_dates: right_date: iteration: 42 20210425
generate_dates: left_date: iteration: 42 20210419
generate_dates: right_date: iteration: 43 20210418
generate_dates: left_date: iteration: 43 20210412
generate_dates: right_date: iteration: 44 20210411
generate_dates: left_date: iteration: 44 20210405
generate_dates: right_date: iteration: 45 20210404
generate_dates: left_date: iteration: 45 20210329
generate_dates: right_date: iteration: 46 20210328
generate_dates: left_date: iteration: 46 20210322
generate_dates: right_date: iteration: 47 20210321
generate_dates: left_date: iteration: 47 20210315
generate_dates: right_date: iteration: 48 20210314
generate_dates: left_date: iteration: 48 20210308
generate_dates: right_date: iteration: 49 20210307
generate_dates: left_date: iteration: 49 20210301
generate_dates: right_date: iteration: 50 20210228
generate_dates: left_date: iteration: 50 20210222
generate_dates: right_date: iteration: 51 20210221
generate_dates: left_date: iteration: 51 20210215
generate_dates: right_date: iteration: 52 20210214
generate_dates: left_date: iteration: 52 20210208
generate_dates: right_date: iteration: 53 20210207
generate_dates: left_date: iteration: 53 20210201
generate_dates: right_date: iteration: 54 20210131
generate_dates: left_date: iteration: 54 20210125
generate_dates: right_date: iteration: 55 20210124
generate_dates: left_date: iteration: 55 20210118
generate_dates: right_date: iteration: 56 20210117
generate_dates: left_date: iteration: 56 20210111
generate_dates: right_date: iteration: 57 20210110
generate_dates: left_date: iteration: 57 20210104
generate_dates: right_date: iteration: 58 20210103
generate_dates: left_date: iteration: 58 20201228
[Stage 5:>                                                          (0 + 1) / 1]Traceback (most recent call last):
  File "extract_and_deliver_telco_sms_indicators.py", line 184, in <module>
    main()
  File "extract_and_deliver_telco_sms_indicators.py", line 178, in main
    extract_and_copyfile(args, spark, td_driver, td_url, td_user, td_password, sdate, edate, prefix)
  File "extract_and_deliver_telco_sms_indicators.py", line 102, in extract_and_copyfile
    extract_sms_telco_data(spark, td_driver, td_url, td_user, td_password, None, categorie, start_date, end_date, filename)
  File "extract_and_deliver_telco_sms_indicators.py", line 93, in extract_sms_telco_data
    agg_data.toPandas().to_csv(output_path, sep=",", index=False)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py", line 141, in toPandas
    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/sql/dataframe.py", line 677, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/py4j/java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.PythonException: 
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 211, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 132, in dump_stream
    for obj in iterator:
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 200, in _batched
    for item in iterator:
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 450, in mapper
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 85, in <lambda>
    return lambda *a: f(*a)
  File "/home/patrice.nzi/backup/oba/obavenv/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py", line 73, in wrapper
    return f(*args, **kwargs)
  File "extract_and_deliver_telco_sms_indicators.py", line 21, in format_number
    is_civ = str_nbr.find("225")
NameError: name 'str_nbr' is not defined

